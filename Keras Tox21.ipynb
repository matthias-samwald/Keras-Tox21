{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras and SELU multilayer perceptrons on Tox21\n",
    "====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io\n",
    "    \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, concatenate\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.utils \n",
    "\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"/media/matthias/Big-disk/tox21-hochreiter/\"\n",
    "\n",
    "# load dense features and labels\n",
    "y_tr = pd.read_csv(source_directory + 'tox21_labels_train.csv.gz', index_col=0, compression=\"gzip\")\n",
    "y_te = pd.read_csv(source_directory + 'tox21_labels_test.csv.gz', index_col=0, compression=\"gzip\")\n",
    "x_tr_dense = pd.read_csv(source_directory + 'tox21_dense_train.csv.gz', index_col=0, compression=\"gzip\").values\n",
    "x_te_dense = pd.read_csv(source_directory + 'tox21_dense_test.csv.gz', index_col=0, compression=\"gzip\").values\n",
    "\n",
    "# load sparse features\n",
    "x_tr_sparse = io.mmread(source_directory + 'tox21_sparse_train.mtx.gz').tocsc()\n",
    "x_te_sparse = io.mmread(source_directory + 'tox21_sparse_test.mtx.gz').tocsc()\n",
    "\n",
    "# filter out very sparse features. combine filtered sparse features with dense features\n",
    "sparse_col_idx = ((x_tr_sparse > 0).mean(0) > 0.05).A.ravel()\n",
    "x_tr = np.hstack([x_tr_dense, x_tr_sparse[:, sparse_col_idx].A])\n",
    "x_te = np.hstack([x_te_dense, x_te_sparse[:, sparse_col_idx].A])\n",
    "\n",
    "# scale input features to zero mean and unit variance (might be important for SELU network)\n",
    "scaler = preprocessing.StandardScaler().fit(x_tr)\n",
    "x_tr = scaler.transform(x_tr)\n",
    "x_te = scaler.transform(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data(target):\n",
    "    \"\"\"\n",
    "    Returns data for a specific assays (i.e., samples where result for assay is available)\n",
    "    \"\"\"\n",
    "    \n",
    "    # filter out data where no results for target assay are available    \n",
    "    rows_tr = np.isfinite(y_tr[target]).values\n",
    "    rows_te = np.isfinite(y_te[target]).values\n",
    "    \n",
    "    x_train = x_tr[rows_tr] \n",
    "    y_train = y_tr[target][rows_tr]\n",
    "    x_test = x_te[rows_te]\n",
    "    y_test = y_te[target][rows_te]   \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "def dense_column (input_node, units, layers, dropout):\n",
    "    nn = input_node\n",
    "    for i in range(layers):\n",
    "        nn = AlphaDropout(dropout)(nn)\n",
    "        nn = Dense(units, activation='selu', kernel_initializer='lecun_normal')(nn)\n",
    "    return nn\n",
    "\n",
    "def accessory_columns_block(input_node, number_of_columns = 10, units_per_column = 20, layers = 5, dropout = 0.03):\n",
    "    nn = input_node\n",
    "    columns = []\n",
    "    for i in range(number_of_columns):\n",
    "        column = dense_column(nn, units_per_column, layers, dropout)\n",
    "        columns += [column]\n",
    "    nn = concatenate(columns)\n",
    "    return nn\n",
    "\n",
    "def f_nn(p):\n",
    "    # add constant parameters to parameter dictionary (workaround for hyperopt, take care to set global variable!)\n",
    "    p = {**p, **constant_parameters}\n",
    "    \n",
    "    #print(\"\\n\")\n",
    "    #print(p)\n",
    "    \n",
    "    try:\n",
    "        max_epochs = 1000\n",
    "\n",
    "        # some parameters need to be cast to integer\n",
    "        p['hidden layers'] = int(p['hidden layers'])\n",
    "        p['hidden units'] = int(p['hidden units'])\n",
    "        p['batch size'] = int(p['batch size'])\n",
    "\n",
    "        number_of_features = x_train.shape[1]\n",
    "        \n",
    "        # clear session (to avoid clutter from old models)\n",
    "        K.clear_session()\n",
    "        \n",
    "        # the model is a classical multilayer perceptron with SELU activations and Alpha dropout\n",
    "        inputs = Input(shape=(number_of_features,))\n",
    "        nn = Dense(p['hidden units'], activation='selu', kernel_initializer=p['kernel initializer'])(inputs)\n",
    "                \n",
    "        for i in range(p['hidden layers']):\n",
    "            if p['shape'] == 'triangle':\n",
    "                # unit decrement per layer if triangle shape is selected, last hidden layer has 4 units\n",
    "                difference = max(p['hidden units'] - 4, 4)\n",
    "                decrement_per_layer = difference / p['hidden layers']\n",
    "                number_of_units_in_this_layer = int(p['hidden units'] - (decrement_per_layer * i))\n",
    "            else:\n",
    "                number_of_units_in_this_layer = p['hidden units']\n",
    "            nn = AlphaDropout(p['dropout rate'])(nn)\n",
    "            nn = Dense(number_of_units_in_this_layer, activation='selu', kernel_initializer=p['kernel initializer'])(nn)\n",
    "        \n",
    "        if p['skip_connection']:\n",
    "            # add additional layer before output layer, feed in original input\n",
    "            nn = concatenate([nn, inputs])\n",
    "            nn = Dense(p['hidden units'], activation='selu', kernel_initializer=p['kernel initializer'])(nn)\n",
    "        \n",
    "        outputs = Dense(1, activation='sigmoid')(nn)\n",
    "\n",
    "        optimizer=optimizers.sgd(lr=p['learning rate'], momentum=0.1, nesterov=True)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['binary_accuracy'])\n",
    "        \n",
    "        # model.summary()\n",
    "        # display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
    "\n",
    "        # weigh classes (in case labels are unbalanced)\n",
    "        class_weight = sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "        _, tmpfn = tempfile.mkstemp()\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=p['patience']), \n",
    "                     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-06, verbose=1),\n",
    "                     ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\n",
    "        model.fit(x_train, y_train, epochs=max_epochs, batch_size=p['batch size'], validation_data=(x_test, y_test), \n",
    "              callbacks=callbacks, class_weight=class_weight, verbose=p['verbose'])\n",
    "\n",
    "        # load model weights of epoch that had best results\n",
    "        model.load_weights(tmpfn)\n",
    "\n",
    "        p_te = model.predict(x_test)  \n",
    "\n",
    "        # add column for other class, adding up to probability of 1\n",
    "        new_col = np.subtract(1,p_te).reshape((p_te.shape[0],1))\n",
    "        p_te_both_classes = np.append(new_col,p_te, 1)\n",
    "\n",
    "        auc_te = roc_auc_score(y_test, p_te_both_classes[:, 1])\n",
    "        print(\"%15s score on test: %3.5f\" % (target, auc_te))\n",
    "\n",
    "        return {'loss': -auc_te, 'status': STATUS_OK, 'model': model}\n",
    "    \n",
    "    except tf.errors.ResourceExhaustedError: \n",
    "        # fail gracefully if model is too large to fit into memory (so hyperparameter search can continue)\n",
    "        print(\"ResourceExhaustedError occurred, skipping...\")\n",
    "        return {'status': STATUS_FAIL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.019999999552965164.\n",
      "         NR.AhR score on test: 0.86605\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.019999999552965164.\n",
      "          NR.AR score on test: 0.80967\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.019999999552965164.\n",
      "      NR.AR.LBD score on test: 0.75719\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00011: reducing learning rate to 0.019999999552965164.\n",
      "   NR.Aromatase score on test: 0.77411\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00014: reducing learning rate to 0.019999999552965164.\n",
      "          NR.ER score on test: 0.78853\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.019999999552965164.\n",
      "      NR.ER.LBD score on test: 0.68043\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00017: reducing learning rate to 0.019999999552965164.\n",
      "  NR.PPAR.gamma score on test: 0.82983\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00017: reducing learning rate to 0.019999999552965164.\n",
      "\n",
      "Epoch 00022: reducing learning rate to 0.009999999776482582.\n",
      "         SR.ARE score on test: 0.77296\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.019999999552965164.\n",
      "       SR.ATAD5 score on test: 0.82751\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.019999999552965164.\n",
      "         SR.HSE score on test: 0.81981\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00017: reducing learning rate to 0.019999999552965164.\n",
      "         SR.MMP score on test: 0.91308\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.03999999910593033.\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.019999999552965164.\n",
      "         SR.p53 score on test: 0.83033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.80579009497722787"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_parameters = {\n",
    "        'shape': 'square',\n",
    "        'optimizer': 'sgd',\n",
    "        'kernel initializer': 'lecun_normal',\n",
    "        'accessory columns': False,\n",
    "        'patience': 10,\n",
    "        'verbose': 0\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'batch size': 100.0, \n",
    "    'dropout rate': 0.05, \n",
    "    'hidden layers': 6.0, \n",
    "    'hidden units': 200.0, \n",
    "    'learning rate': 8e-02,\n",
    "    'skip_connection': False\n",
    "}\n",
    "\n",
    "scores = np.array([])\n",
    "\n",
    "for target in ['NR.AhR', 'NR.AR', 'NR.AR.LBD', 'NR.Aromatase', 'NR.ER', 'NR.ER.LBD', \n",
    "               'NR.PPAR.gamma', 'SR.ARE', 'SR.ATAD5', 'SR.HSE', 'SR.MMP', 'SR.p53']:\n",
    "    target = target\n",
    "    x_train, y_train, x_test, y_test = data(target)\n",
    "    result = f_nn(parameters)\n",
    "    scores = np.append(scores, -result['loss'])\n",
    "\n",
    "np.average(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
